# GPT-3.5-Turbo Machine Translation Evaluation

## Overview
This repository contains the code and data for evaluating machine translations generated by OpenAI's GPT-3.5-Turbo model using parallel corpora of English texts from various fields (literature, medical, law) and their corresponding human translations to Russian. The evaluation aims to determine the effectiveness of different prompts in generating accurate translations and to identify which prompt performs better in each field.

## Methodology
1. **Data Collection**: 
   - Parallel corpora of English texts and their human translations to Russian were collected from diverse fields including literature, medical, and law.

2. **Machine Translation**:
   - GPT-3.5-Turbo was employed for machine translation from English to Russian using two different prompts.

3. **Evaluation**:
   - The quality of machine translations was assessed using the BLEU and COMET metrics, comparing machine translations against human translations.

4. **Analysis**:
   - Statistical analysis was performed to compare the BLEU and COMET scores obtained from different prompts across various fields, aiming to determine the most effective prompt for each field.
  
## Usage
- Clone the repository
  ```sh
  git clone https://github.com/svetaku/MT_Evaluation.git
  cd MT_Evaluation
- Install dependencies
  ```sh
  pip install -r requirements.txt
- Prepare the dataset. The project requires a dataset in a JSON file (texts.json). Users can create their own JSON file with their own dataset. Ensure the JSON file follows this structure:
  ```sh
  [{
  "field": "Text genre/field",
  "title": "Text title",
  "en-text": "English text here",
  "ru_human_translation": "Russian reference (human) translation",
  "en_length": "Word count of the English text"
      },
  // Add more entries as needed
   }]

- To use your OpenAI API key, create a `.env` file in the root directory of your project. Insert your key/value pairs in the following format: \
`OPENAI_API_KEY="your openai api key"`
- You can now open and run the Jupyter notebooks to create and evaluate machine translations:
   - 01_machine_translation_with_openai.ipynb - This notebook is used for creating machine translations for the dataset.
   - 02_bleu_vs_comet.ipynb - This notebook is used for evaluating the translations using BLEU and COMET scores.
