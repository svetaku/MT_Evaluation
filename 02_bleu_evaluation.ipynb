{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1 \n",
      " You are an expert translator. Translate the following text to Russian using vocabulary and expressions of a Russian native. The text to be translated is:\n",
      "Prompt 2 \n",
      " You are an expert translator that will be tasked with translating a piece of text into Russian. The translation must be faithful to the original tone of voice and writing style. Ensure that the meaning of the original text is not changed. The text to be translated is:\n"
     ]
    }
   ],
   "source": [
    "# reading the reference-candidate dataset and printing the prompts used for translation \n",
    "\n",
    "file_path = 'reference_candidate.json'\n",
    "\n",
    "with open(file_path, 'r') as json_file:\n",
    "    translation_data = json.load(json_file)\n",
    "\n",
    "prompts = set()\n",
    "\n",
    "for label in translation_data.values():\n",
    "    for prompt in label:\n",
    "        prompts.add(prompt)\n",
    "\n",
    "prompt_names = {}\n",
    "for num, prompt in enumerate(prompts):\n",
    "     prompt_name = 'Prompt '+str(num+1)\n",
    "     prompt_names[prompt] = prompt_name\n",
    "     print(prompt_name, '\\n', prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists of values from the dataset for evaluation\n",
    "\n",
    "output = []\n",
    "\n",
    "# Iterate through each label\n",
    "for label, prompts in translation_data.items():\n",
    "    # Iterate through each prompt in the label\n",
    "    for prompt, translations in prompts.items():\n",
    "        human_translations = []\n",
    "        machine_translations = []\n",
    "        # Iterate through each translation and separate human and machine translations\n",
    "        for translation in translations:\n",
    "            human_translations.append(translation[0])\n",
    "            machine_translations.append(translation[1])\n",
    "\n",
    "        # Append to the output list\n",
    "        output.append([label, prompt_names[prompt], [human_translations, machine_translations]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: literature\n",
      "Prompt 1\n",
      "BLEU score: 0.1479391363697395\n",
      "Precisions: [0.4540755467196819, 0.19491525423728814, 0.10158730158730159, 0.05704534373476353]\n",
      "Brevity penalty: 0.9830479158592157\n",
      "Length ratio: 0.9831899921813917 \n",
      "\n",
      "label: literature\n",
      "Prompt 2\n",
      "BLEU score: 0.1476482987962879\n",
      "Precisions: [0.4513970877607241, 0.1922110552763819, 0.09932885906040269, 0.056195965417867436]\n",
      "Brevity penalty: 0.9952885835296004\n",
      "Length ratio: 0.9952996474735605 \n",
      "\n",
      "label: medical\n",
      "Prompt 1\n",
      "BLEU score: 0.20121532850099372\n",
      "Precisions: [0.47480482611781405, 0.2545045045045045, 0.15219123505976095, 0.08913412563667232]\n",
      "Brevity penalty: 1.0\n",
      "Length ratio: 1.0429311621021466 \n",
      "\n",
      "label: medical\n",
      "Prompt 2\n",
      "BLEU score: 0.19289480479294646\n",
      "Precisions: [0.4764089121887287, 0.24740124740124741, 0.14705882352941177, 0.07987470634299139]\n",
      "Brevity penalty: 1.0\n",
      "Length ratio: 1.0423497267759563 \n",
      "\n",
      "label: law\n",
      "Prompt 1\n",
      "BLEU score: 0.17864525092905062\n",
      "Precisions: [0.4863169897377423, 0.24211778703152886, 0.1449004975124378, 0.08836907082521117]\n",
      "Brevity penalty: 0.9065930798844307\n",
      "Length ratio: 0.9106957424714434 \n",
      "\n",
      "label: law\n",
      "Prompt 2\n",
      "BLEU score: 0.17643819680092637\n",
      "Precisions: [0.4808584686774942, 0.24031476997578693, 0.14041745730550284, 0.083223249669749]\n",
      "Brevity penalty: 0.9204002623969358\n",
      "Length ratio: 0.9234065345474023 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating the metrics\n",
    "\n",
    "for item in output:\n",
    "    print('label:', item[0])\n",
    "    print(item[1])\n",
    "    references = item[2][0]\n",
    "    predictions = item[2][1]\n",
    "    results = bleu.compute(predictions=predictions, references=references)\n",
    "    print('BLEU score:', results['bleu'])\n",
    "    print('Precisions:', results['precisions'])\n",
    "    print('Brevity penalty:', results['brevity_penalty'])\n",
    "    print('Length ratio:', results['length_ratio'], '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
